"""
Módulo de Backtesting para Forecast MR

Proporciona herramientas para evaluar la precisión real de los modelos
mediante simulación de predicciones históricas.

Incluye:
- Walk-forward validation
- Comparación predicciones vs valores reales
- Métricas agregadas por período
- Visualización de resultados

Author: Manuel Remón
"""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Tuple, Callable
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from loguru import logger
import warnings

warnings.filterwarnings('ignore')


@dataclass
class BacktestStep:
    """Resultado de un paso individual de backtesting"""
    fecha_corte: datetime
    fecha_inicio_test: datetime
    fecha_fin_test: datetime
    n_train: int
    n_test: int
    predicciones: pd.Series
    valores_reales: pd.Series
    mae: float
    rmse: float
    mape: float
    r2: float

    def to_dict(self) -> Dict[str, Any]:
        return {
            'fecha_corte': self.fecha_corte.isoformat(),
            'fecha_inicio_test': self.fecha_inicio_test.isoformat(),
            'fecha_fin_test': self.fecha_fin_test.isoformat(),
            'n_train': self.n_train,
            'n_test': self.n_test,
            'mae': round(self.mae, 2),
            'rmse': round(self.rmse, 2),
            'mape': round(self.mape, 2) if not np.isinf(self.mape) else None,
            'r2': round(self.r2, 4)
        }


@dataclass
class BacktestReport:
    """Reporte completo de backtesting"""
    modelo_tipo: str
    steps: List[BacktestStep]
    ventana_test: int
    n_pasos: int
    metricas_agregadas: Dict[str, float]
    metricas_por_periodo: pd.DataFrame
    timestamp: datetime = field(default_factory=datetime.now)

    @property
    def mae_promedio(self) -> float:
        return self.metricas_agregadas.get('mae_mean', 0)

    @property
    def mae_std(self) -> float:
        return self.metricas_agregadas.get('mae_std', 0)

    @property
    def es_estable(self) -> bool:
        """Modelo es estable si el coeficiente de variación de MAE < 0.5"""
        if self.mae_promedio == 0:
            return True
        cv = self.mae_std / self.mae_promedio
        return cv < 0.5

    def to_dict(self) -> Dict[str, Any]:
        return {
            'modelo_tipo': self.modelo_tipo,
            'ventana_test': self.ventana_test,
            'n_pasos': self.n_pasos,
            'metricas_agregadas': {k: round(v, 4) for k, v in self.metricas_agregadas.items()},
            'es_estable': self.es_estable,
            'steps': [s.to_dict() for s in self.steps],
            'timestamp': self.timestamp.isoformat()
        }

    def get_predicciones_vs_reales(self) -> pd.DataFrame:
        """Retorna DataFrame con todas las predicciones vs valores reales"""
        rows = []
        for step in self.steps:
            for i, (pred, real) in enumerate(zip(step.predicciones, step.valores_reales)):
                rows.append({
                    'fecha_corte': step.fecha_corte,
                    'dia': i + 1,
                    'prediccion': pred,
                    'real': real,
                    'error': abs(pred - real),
                    'error_pct': abs(pred - real) / real * 100 if real != 0 else 0
                })
        return pd.DataFrame(rows)


class Backtester:
    """
    Ejecutor de backtesting para modelos de forecast.

    Implementa walk-forward validation para evaluar la precisión
    real de los modelos en datos históricos.

    Ejemplo de uso:
        backtester = Backtester(predictor)
        report = backtester.ejecutar(
            df=df_historico,
            ventana_test=30,
            n_pasos=5
        )

        print(f"MAE promedio: {report.mae_promedio:.2f}")
        print(f"Modelo estable: {report.es_estable}")
    """

    def __init__(self, predictor_class, modelo_tipo: str = 'random_forest'):
        """
        Inicializa el backtester.

        Args:
            predictor_class: Clase del predictor a usar (DemandPredictor)
            modelo_tipo: Tipo de modelo a evaluar
        """
        self.predictor_class = predictor_class
        self.modelo_tipo = modelo_tipo

    def ejecutar(
        self,
        df: pd.DataFrame,
        columna_fecha: str = 'fecha',
        columna_cantidad: str = 'cantidad',
        ventana_test: int = 30,
        n_pasos: int = 5,
        min_train: int = 60,
        stride: Optional[int] = None
    ) -> BacktestReport:
        """
        Ejecuta backtesting walk-forward.

        Args:
            df: DataFrame con datos históricos
            columna_fecha: Nombre de columna de fechas
            columna_cantidad: Nombre de columna de cantidades
            ventana_test: Tamaño de ventana de test (días)
            n_pasos: Número de pasos de backtesting
            min_train: Mínimo de días para entrenamiento
            stride: Días entre cada paso (default: ventana_test)

        Returns:
            BacktestReport con resultados completos
        """
        logger.info(f"Iniciando backtesting: {n_pasos} pasos, ventana={ventana_test} días")

        # Preparar datos
        df_prep = self._preparar_datos(df, columna_fecha, columna_cantidad)

        if len(df_prep) < min_train + ventana_test:
            raise ValueError(
                f"Datos insuficientes para backtesting. "
                f"Necesita al menos {min_train + ventana_test} días, tiene {len(df_prep)}"
            )

        # Calcular fechas de corte
        stride = stride or ventana_test
        fechas_corte = self._calcular_fechas_corte(
            df_prep, columna_fecha, ventana_test, n_pasos, min_train, stride
        )

        # Ejecutar cada paso
        steps = []
        for i, fecha_corte in enumerate(fechas_corte):
            logger.debug(f"Paso {i+1}/{len(fechas_corte)}: corte en {fecha_corte}")

            try:
                step = self._ejecutar_paso(
                    df_prep, fecha_corte, columna_fecha, columna_cantidad, ventana_test
                )
                steps.append(step)
            except Exception as e:
                logger.warning(f"Error en paso {i+1}: {e}")
                continue

        if not steps:
            raise ValueError("No se completó ningún paso de backtesting")

        # Calcular métricas agregadas
        metricas_agregadas = self._calcular_metricas_agregadas(steps)

        # Crear DataFrame de métricas por período
        metricas_por_periodo = self._crear_metricas_por_periodo(steps)

        logger.info(
            f"Backtesting completado: MAE={metricas_agregadas['mae_mean']:.2f} "
            f"(±{metricas_agregadas['mae_std']:.2f})"
        )

        return BacktestReport(
            modelo_tipo=self.modelo_tipo,
            steps=steps,
            ventana_test=ventana_test,
            n_pasos=len(steps),
            metricas_agregadas=metricas_agregadas,
            metricas_por_periodo=metricas_por_periodo
        )

    def _preparar_datos(
        self,
        df: pd.DataFrame,
        columna_fecha: str,
        columna_cantidad: str
    ) -> pd.DataFrame:
        """Prepara y valida los datos para backtesting"""
        df_prep = df.copy()

        # Asegurar tipos correctos
        df_prep[columna_fecha] = pd.to_datetime(df_prep[columna_fecha])
        df_prep[columna_cantidad] = pd.to_numeric(df_prep[columna_cantidad], errors='coerce')

        # Ordenar por fecha
        df_prep = df_prep.sort_values(columna_fecha).reset_index(drop=True)

        # Eliminar nulos
        df_prep = df_prep.dropna(subset=[columna_fecha, columna_cantidad])

        return df_prep

    def _calcular_fechas_corte(
        self,
        df: pd.DataFrame,
        columna_fecha: str,
        ventana_test: int,
        n_pasos: int,
        min_train: int,
        stride: int
    ) -> List[datetime]:
        """Calcula las fechas de corte para backtesting"""
        fecha_max = df[columna_fecha].max()
        fecha_min = df[columna_fecha].min()

        fechas = []
        fecha_actual = fecha_max - timedelta(days=ventana_test)

        while len(fechas) < n_pasos:
            # Verificar que hay suficientes datos de entrenamiento
            datos_train = df[df[columna_fecha] < fecha_actual]
            if len(datos_train) < min_train:
                break

            fechas.append(fecha_actual)
            fecha_actual = fecha_actual - timedelta(days=stride)

        # Invertir para que vayan de más antiguo a más reciente
        return list(reversed(fechas))

    def _ejecutar_paso(
        self,
        df: pd.DataFrame,
        fecha_corte: datetime,
        columna_fecha: str,
        columna_cantidad: str,
        ventana_test: int
    ) -> BacktestStep:
        """Ejecuta un paso individual de backtesting"""
        # Dividir datos
        df_train = df[df[columna_fecha] < fecha_corte].copy()
        df_test = df[
            (df[columna_fecha] >= fecha_corte) &
            (df[columna_fecha] < fecha_corte + timedelta(days=ventana_test))
        ].copy()

        if len(df_test) == 0:
            raise ValueError(f"No hay datos de test para fecha {fecha_corte}")

        # Agregar por fecha (en caso de múltiples registros por día)
        df_train_agg = df_train.groupby(columna_fecha)[columna_cantidad].sum().reset_index()
        df_test_agg = df_test.groupby(columna_fecha)[columna_cantidad].sum().reset_index()

        # Entrenar modelo
        predictor = self.predictor_class(modelo=self.modelo_tipo)
        predictor.entrenar(df_train_agg, columna_cantidad)

        # Predecir
        n_dias_test = len(df_test_agg)
        df_pred = predictor.predecir(df_train_agg, n_dias_test)

        # Alinear predicciones con valores reales
        predicciones = df_pred['prediccion'].values[:len(df_test_agg)]
        valores_reales = df_test_agg[columna_cantidad].values[:len(predicciones)]

        # Calcular métricas
        mae = self._calcular_mae(predicciones, valores_reales)
        rmse = self._calcular_rmse(predicciones, valores_reales)
        mape = self._calcular_mape(predicciones, valores_reales)
        r2 = self._calcular_r2(predicciones, valores_reales)

        return BacktestStep(
            fecha_corte=fecha_corte,
            fecha_inicio_test=df_test_agg[columna_fecha].min(),
            fecha_fin_test=df_test_agg[columna_fecha].max(),
            n_train=len(df_train_agg),
            n_test=len(df_test_agg),
            predicciones=pd.Series(predicciones),
            valores_reales=pd.Series(valores_reales),
            mae=mae,
            rmse=rmse,
            mape=mape,
            r2=r2
        )

    def _calcular_mae(self, pred: np.ndarray, real: np.ndarray) -> float:
        """Calcula Mean Absolute Error"""
        return float(np.mean(np.abs(pred - real)))

    def _calcular_rmse(self, pred: np.ndarray, real: np.ndarray) -> float:
        """Calcula Root Mean Squared Error"""
        return float(np.sqrt(np.mean((pred - real) ** 2)))

    def _calcular_mape(self, pred: np.ndarray, real: np.ndarray) -> float:
        """Calcula Mean Absolute Percentage Error"""
        mask = real != 0
        if not mask.any():
            return np.inf
        return float(np.mean(np.abs((pred[mask] - real[mask]) / real[mask])) * 100)

    def _calcular_r2(self, pred: np.ndarray, real: np.ndarray) -> float:
        """Calcula R-squared"""
        ss_res = np.sum((real - pred) ** 2)
        ss_tot = np.sum((real - np.mean(real)) ** 2)
        if ss_tot == 0:
            return 0.0
        return float(1 - (ss_res / ss_tot))

    def _calcular_metricas_agregadas(self, steps: List[BacktestStep]) -> Dict[str, float]:
        """Calcula métricas agregadas de todos los pasos"""
        maes = [s.mae for s in steps]
        rmses = [s.rmse for s in steps]
        mapes = [s.mape for s in steps if not np.isinf(s.mape)]
        r2s = [s.r2 for s in steps]

        return {
            'mae_mean': np.mean(maes),
            'mae_std': np.std(maes),
            'mae_min': np.min(maes),
            'mae_max': np.max(maes),
            'rmse_mean': np.mean(rmses),
            'rmse_std': np.std(rmses),
            'mape_mean': np.mean(mapes) if mapes else np.inf,
            'mape_std': np.std(mapes) if mapes else 0,
            'r2_mean': np.mean(r2s),
            'r2_std': np.std(r2s),
            'n_pasos_exitosos': len(steps)
        }

    def _crear_metricas_por_periodo(self, steps: List[BacktestStep]) -> pd.DataFrame:
        """Crea DataFrame con métricas por período"""
        rows = []
        for step in steps:
            rows.append({
                'fecha_corte': step.fecha_corte,
                'n_train': step.n_train,
                'n_test': step.n_test,
                'mae': step.mae,
                'rmse': step.rmse,
                'mape': step.mape,
                'r2': step.r2
            })
        return pd.DataFrame(rows)


class ModelComparator:
    """
    Compara múltiples modelos usando backtesting.

    Ejemplo de uso:
        comparator = ModelComparator(predictor_class)
        comparison = comparator.comparar(
            df=df_historico,
            modelos=['random_forest', 'gradient_boosting', 'linear']
        )

        print(f"Mejor modelo: {comparison['mejor_modelo']}")
    """

    def __init__(self, predictor_class):
        """
        Inicializa el comparador.

        Args:
            predictor_class: Clase del predictor (DemandPredictor)
        """
        self.predictor_class = predictor_class

    def comparar(
        self,
        df: pd.DataFrame,
        modelos: List[str],
        columna_fecha: str = 'fecha',
        columna_cantidad: str = 'cantidad',
        ventana_test: int = 30,
        n_pasos: int = 3
    ) -> Dict[str, Any]:
        """
        Compara múltiples modelos usando backtesting.

        Args:
            df: DataFrame con datos históricos
            modelos: Lista de tipos de modelo a comparar
            columna_fecha: Nombre de columna de fechas
            columna_cantidad: Nombre de columna de cantidades
            ventana_test: Tamaño de ventana de test
            n_pasos: Número de pasos de backtesting

        Returns:
            Diccionario con comparación y mejor modelo
        """
        logger.info(f"Comparando {len(modelos)} modelos")

        resultados = {}
        for modelo in modelos:
            try:
                backtester = Backtester(self.predictor_class, modelo)
                report = backtester.ejecutar(
                    df, columna_fecha, columna_cantidad,
                    ventana_test, n_pasos
                )
                resultados[modelo] = {
                    'mae_mean': report.mae_promedio,
                    'mae_std': report.mae_std,
                    'rmse_mean': report.metricas_agregadas['rmse_mean'],
                    'r2_mean': report.metricas_agregadas['r2_mean'],
                    'es_estable': report.es_estable,
                    'report': report
                }
                logger.info(f"  {modelo}: MAE={report.mae_promedio:.2f}")
            except Exception as e:
                logger.warning(f"Error evaluando {modelo}: {e}")
                resultados[modelo] = {
                    'error': str(e),
                    'mae_mean': np.inf
                }

        # Encontrar mejor modelo
        modelos_validos = {k: v for k, v in resultados.items() if 'error' not in v}
        if modelos_validos:
            mejor_modelo = min(modelos_validos.keys(), key=lambda x: modelos_validos[x]['mae_mean'])
        else:
            mejor_modelo = None

        # Generar recomendación
        recomendacion = self._generar_recomendacion(resultados, mejor_modelo)

        return {
            'resultados': resultados,
            'mejor_modelo': mejor_modelo,
            'recomendacion': recomendacion,
            'ranking': self._crear_ranking(resultados)
        }

    def _crear_ranking(self, resultados: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Crea ranking de modelos por MAE"""
        ranking = []
        for modelo, datos in resultados.items():
            if 'error' not in datos:
                ranking.append({
                    'modelo': modelo,
                    'mae': datos['mae_mean'],
                    'rmse': datos['rmse_mean'],
                    'r2': datos['r2_mean'],
                    'estable': datos['es_estable']
                })

        ranking.sort(key=lambda x: x['mae'])
        for i, item in enumerate(ranking):
            item['posicion'] = i + 1

        return ranking

    def _generar_recomendacion(
        self,
        resultados: Dict[str, Any],
        mejor_modelo: Optional[str]
    ) -> str:
        """Genera recomendación basada en resultados"""
        if not mejor_modelo:
            return "No se pudo determinar el mejor modelo. Verifique los datos."

        datos_mejor = resultados[mejor_modelo]

        partes = [f"Modelo recomendado: {mejor_modelo}"]

        mae = datos_mejor['mae_mean']
        r2 = datos_mejor['r2_mean']

        if r2 > 0.8:
            partes.append(f"Excelente ajuste (R²={r2:.2f}).")
        elif r2 > 0.6:
            partes.append(f"Buen ajuste (R²={r2:.2f}).")
        elif r2 > 0.4:
            partes.append(f"Ajuste moderado (R²={r2:.2f}). Considere más datos.")
        else:
            partes.append(f"Ajuste débil (R²={r2:.2f}). Los datos pueden ser muy ruidosos.")

        if datos_mejor['es_estable']:
            partes.append("El modelo es estable entre períodos.")
        else:
            partes.append("El modelo muestra variabilidad. Considere más datos de entrenamiento.")

        return " ".join(partes)


def ejecutar_backtest_rapido(
    df: pd.DataFrame,
    predictor_class,
    modelo: str = 'random_forest',
    ventana: int = 30,
    pasos: int = 3
) -> BacktestReport:
    """
    Función de conveniencia para ejecutar backtesting rápido.

    Args:
        df: DataFrame con datos
        predictor_class: Clase del predictor
        modelo: Tipo de modelo
        ventana: Días de ventana de test
        pasos: Número de pasos

    Returns:
        BacktestReport con resultados
    """
    backtester = Backtester(predictor_class, modelo)
    return backtester.ejecutar(df, ventana_test=ventana, n_pasos=pasos)


def comparar_modelos_rapido(
    df: pd.DataFrame,
    predictor_class,
    modelos: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Función de conveniencia para comparar modelos rápidamente.

    Args:
        df: DataFrame con datos
        predictor_class: Clase del predictor
        modelos: Lista de modelos (default: rf, gb, linear)

    Returns:
        Diccionario con comparación
    """
    modelos = modelos or ['random_forest', 'gradient_boosting', 'linear']
    comparator = ModelComparator(predictor_class)
    return comparator.comparar(df, modelos)
